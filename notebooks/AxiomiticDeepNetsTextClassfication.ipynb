{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AxiomiticDeepNetsTextClassfication.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qR0OtlkVqih"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjlrSP8LVuSY"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import urllib\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVCuvxAQWV3M"
      },
      "source": [
        "#Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r77vtJ_ox_C"
      },
      "source": [
        "def interpolate(baseline, input, steps,plot=False):\n",
        "  assert input.shape[1]==baseline.shape[1]\n",
        "  interpolates=torch.empty((steps,*input.shape))\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for idx in range(steps):\n",
        "      alpha=idx/steps\n",
        "      interpolated=baseline+(alpha*(input-baseline))\n",
        "      if plot:\n",
        "        plt.subplot(int(steps/2),steps-int(steps/2),idx+1)\n",
        "       \n",
        "        plt.imshow(transforms.ToPILImage()(interpolated))\n",
        " \n",
        "      interpolates[idx,...]=interpolated\n",
        "  \n",
        "  return interpolates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figbBPREXGlH"
      },
      "source": [
        "#Gradiant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6STdA1bYfzq"
      },
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def computeGradiant(input,model,target=None):\n",
        "          gradient=torch.empty_like(input)\n",
        "          \n",
        "          #model.eval()\n",
        "          model.to(device)\n",
        "\n",
        "          model.freeze=True\n",
        "          model.zero_grad()\n",
        "        \n",
        "          input_batch = input.unsqueeze(0)\n",
        "      \n",
        "          input_batch=input_batch.permute(1,0,2)\n",
        "          \n",
        "          output=model(input_batch.to(device),text_lengths=lengthTensor)\n",
        "          \n",
        "          output=torch.sigmoid(output)\n",
        "          \n",
        "          gradient=torch.autograd.grad(output,inputs= input_batch,)[0]\n",
        "          #return gradient.squeeze_(0)\n",
        "          return gradient.permute(1,0,2).squeeze(0).detach().cpu()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fAKA-SLXKPY"
      },
      "source": [
        "#Integrated Gradiant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f2YkjbSYgwO"
      },
      "source": [
        "#class model , n_steps , internal_batch_size , method\n",
        "#Methods : explain return attributions , parameters X , baseline , target\n",
        "def generate_IG(input, baseline,model,n_steps,target_idx):\n",
        "  norm=input-baseline\n",
        "  interpol=interpolate(baseline, input, n_steps)\n",
        "  gradient=torch.empty(*interpol.shape)\n",
        "  for idx,i in enumerate(interpol):\n",
        "    gradient[idx,...]=computeGradiant(i,model,target_idx)\n",
        "    gradient=gradient.to(norm.device)\n",
        "\n",
        "  IG=torch.mean(gradient[:-1],dim=0)*norm\n",
        "  return IG,gradient[-1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o-zt2e_TxyL",
        "outputId": "07e3a16b-ef76-49c5-ab50-f0816890c4a9"
      },
      "source": [
        "!python '/content/drive/MyDrive/AxiomiticDeepNets/main.py'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rLL2prxRAkS"
      },
      "source": [
        "#Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h8qj2u6REbm"
      },
      "source": [
        "#Bring Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIADBeBKRDq9"
      },
      "source": [
        "import torch,torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets \n",
        "import spacy\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ujiI7_HjAKG"
      },
      "source": [
        "TEXT=data.Field(tokenize='spacy',tokenizer_language='en_core_web_sm',include_lengths = True)\n",
        "LABEL=data.LabelField(dtype=torch.float)\n",
        "\n",
        "# make splits for data\n",
        "train, test = datasets.IMDB.splits(TEXT, LABEL,)\n",
        "traindata ,  valid = train.split(split_ratio=0.9)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV_NqPow-pxO"
      },
      "source": [
        "# build the vocabulary\n",
        "TEXT.build_vocab(traindata,unk_init=torch.normal,max_size=25000,vectors = \"glove.6B.100d\")\n",
        "LABEL.build_vocab(traindata)\n",
        "\n",
        "trainLoader,validLoader,testLoader=data.BucketIterator.splits(datasets=(traindata,valid,test),batch_size=64,sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YeABi3-Yf_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0f1fa8-50d6-4411-d83b-4174f09c0692"
      },
      "source": [
        "len(testLoader),len(trainLoader),len(validLoader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(391, 352, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY-c-F_NEDF8"
      },
      "source": [
        "#Model Architucture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe5YYjhhEFZC"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    #define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout,pad_idx,freeze=False):\n",
        "        \n",
        "        #Constructor\n",
        "        super().__init__()          \n",
        "        \n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim,padding_idx = pad_idx)\n",
        "        \n",
        "        #lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,\n",
        "                          )\n",
        "        \n",
        "        #dense layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        #activation function\n",
        "        #self.act = nn.Sigmoid()\n",
        "        self.freeze=freeze\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent_length,batch_size]\n",
        "        if not self.freeze:\n",
        "              embedded = self.dropout(self.embedding(text))\n",
        "             \n",
        "        #embedded = [sent_len,batch size, emb dim]\n",
        "        else:\n",
        "              embedded=self.dropout(text)\n",
        "        #packed sequence\n",
        "        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,  text_lengths.to('cpu'))\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "\n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden =self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs=self.fc(hidden)\n",
        "\n",
        "        #Final activation function\n",
        "        #outputs=self.act(dense_outputs)\n",
        "        \n",
        "        return dense_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttblmn0FE4pc"
      },
      "source": [
        "size_of_vocab = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "num_hidden_nodes = 256\n",
        "num_output_nodes = 1\n",
        "num_layers = 2\n",
        "bidirection = True\n",
        "dropout = 0.2\n",
        "pad_idx=TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "#instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
        "                   bidirectional = True, dropout = dropout,pad_idx=pad_idx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bQ4zn8HE57L"
      },
      "source": [
        "#Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaDtIwnGE0mO"
      },
      "source": [
        "def accuracy(preds,true):\n",
        "  preds=torch.round(torch.sigmoid(preds))\n",
        "  return (preds==true).sum()/len(true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qufwtR4LFHpC"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    #initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    #set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        #resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        #retrieve text and no. of words\n",
        "        text,  text_lengths= batch.text   \n",
        "        \n",
        "        #print(text.shape, text_lengths.shape)\n",
        "        \n",
        "        #convert to 1D tensor\n",
        "        predictions = model(text, text_lengths).squeeze(1)  \n",
        "        #print(predictions)\n",
        "        \n",
        "        #compute the loss\n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        print(loss)\n",
        "        #compute the binary accuracy\n",
        "        acc =accuracy(predictions, batch.label)   \n",
        "        \n",
        "        #backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        #update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        #loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator),epoch_acc / len(iterator)\n",
        "    \n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzC8o73pFWbf"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  epoch_loss=0\n",
        "  epoch_acc=0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      text, text_lengths=batch.text\n",
        "      predictions=model(text, text_lengths).squeeze()  \n",
        "      #compute the loss\n",
        "      loss = criterion(predictions, batch.label)        \n",
        "        \n",
        "        #compute the binary accuracy\n",
        "      acc =accuracy(predictions, batch.label) \n",
        "       #loss and accuracy\n",
        "      epoch_loss += loss.item()  \n",
        "      epoch_acc += acc.item()    \n",
        "        \n",
        "  return epoch_loss / len(iterator),   epoch_acc / len(iterator)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDb7f6c2yDC2"
      },
      "source": [
        "optimizor=torch.optim.Adam(model.parameters())\n",
        "criterion=nn.BCEWithLogitsLoss()\n",
        "model.to(device)\n",
        "best_loss=float('inf')\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "for epoch in range(5):\n",
        "      loss_train, acc_train=train(model, trainLoader, optimizor,criterion)\n",
        "      print(f'train_acc {acc_train}')\n",
        "      loss_valid, acc_valid=evaluate(model, validLoader,criterion)\n",
        "      print(f'valid_acc {acc_valid}')\n",
        "      if loss_valid <best_loss:\n",
        "        best_loss=loss_valid\n",
        "        torch.save(model.state_dict(),'SentimentModel.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP_CQh-KxfTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a57885d-310c-465b-9dd4-ce9822e3d312"
      },
      "source": [
        "model.load_state_dict(torch.load('SentimentModel.pt'))\n",
        "loss_test ,acc_test=evaluate(model, testLoader,criterion)\n",
        "print(f'test_acc {acc_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_acc 0.8902653452685422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q8r7VoRjpJf"
      },
      "source": [
        "torch.save(model.state_dict(),'/content/drive/MyDrive/AxiomiticDeepNets/SentimentModel2.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQUWMIHKwoE8"
      },
      "source": [
        "#Text Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x3MSQoGllAVl",
        "outputId": "8f143373-6434-47de-a8b6-a20c873bc8ad"
      },
      "source": [
        "#Test Interpolation\n",
        "sent='i watched this movie on theater and i did  like it'\n",
        "tokenize=spacy.load('en_core_web_sm')\n",
        "tokens=[tok.text for tok in tokenize.tokenizer(sent)]\n",
        "\n",
        "onehot=[TEXT.vocab.stoi[tok] for tok in tokens]\n",
        "onehot=torch.LongTensor(onehot)\n",
        "\n",
        "embed=model.embedding(onehot.to(device))\n",
        "baseline=torch.zeros_like(embed)\n",
        "interpol=interpolate(baseline,embed,steps=10)\n",
        "\n",
        "lengthTensor=torch.LongTensor([len(onehot)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwysGDP7lsPw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bdc74ca2-98ac-4c88-9884-ab1f5d710ac4"
      },
      "source": [
        "torch.backends.cudnn.enabled = False\n",
        "grad=computeGradiant(interpol[9],model)\n",
        "IG,grads=generate_IG(embed,baseline,model,n_steps=240,target_idx=None)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVxs-xPijwh-"
      },
      "source": [
        "IG=torch.sum(IG,dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXivF5FEwZpF"
      },
      "source": [
        "#Text Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iQPlDaw3cVt"
      },
      "source": [
        "from IPython.display import HTML\n",
        "import matplotlib as mpl\n",
        "def  hlstr(string, color='white'):\n",
        "    \"\"\"\n",
        "    Return HTML markup highlighting text with the desired color.\n",
        "    \"\"\"\n",
        "    return f\"<mark style=background-color:{color}>{string} </mark>\"\n",
        "def colorize(attrs, cmap='PiYG'):\n",
        "    \"\"\"\n",
        "    Compute hex colors based on the attributions for a single instance.\n",
        "    Uses a diverging colorscale by default and normalizes and scales\n",
        "    the colormap so that colors are consistent with the attributions.\n",
        "    \"\"\"\n",
        " \n",
        "    cmap_bound = np.abs(attrs).max()\n",
        "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
        "    cmap = mpl.cm.get_cmap(cmap)\n",
        "\n",
        "    # now compute hex values of colors\n",
        "    colors = list(map(lambda x: mpl.colors.rgb2hex(cmap(norm(x))), attrs))\n",
        "    return colors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGBg_vWD-zh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "109fdde5-c070-4c62-cae8-59f8e3677d86"
      },
      "source": [
        "colors = colorize(IG.detach().cpu().numpy())\n",
        "HTML(\"\".join(list(map(hlstr, tokens, colors))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<mark style=background-color:#cfebaa>i </mark><mark style=background-color:#ebf6db>watched </mark><mark style=background-color:#eeabd2>this </mark><mark style=background-color:#e897c4>movie </mark><mark style=background-color:#fad6ea>on </mark><mark style=background-color:#a7d672>theater </mark><mark style=background-color:#e2f3ca>and </mark><mark style=background-color:#88c24c>i </mark><mark style=background-color:#8e0152>did </mark><mark style=background-color:#f4bfdf>  </mark><mark style=background-color:#e181b5>like </mark><mark style=background-color:#e7f5d3>it </mark>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lofkX6ChwgGC"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqRx4qlbobHT"
      },
      "source": [
        "model.freeze=False\n",
        "onehot=onehot.to(device)\n",
        "onehot.unsqueeze_(1)\n",
        "prediction = torch.sigmoid(model(onehot, lengthTensor))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OvEwZhrpC1l",
        "outputId": "5a8f4f6d-bd84-48d4-9934-992774d239d2"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7586]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk6LkKKNuQ5L"
      },
      "source": [
        "#GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y0VxWQBuS1d",
        "outputId": "58a6b4ab-8205-4e45-e78a-1e7e26f01ab0"
      },
      "source": [
        "%cd '/content/drive/MyDrive/AxiomiticDeepNets'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/AxiomiticDeepNets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFJhNdi9yBfq",
        "outputId": "0719de9d-291e-4fc4-915f-cbe783bb51f0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdxJWSDKuhH5",
        "outputId": "47f8ad32-8742-46cd-fc83-4bd9141e5b98"
      },
      "source": [
        "!git init"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/drive/MyDrive/AxiomiticDeepNets/.git/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87zqeLdTuoif",
        "outputId": "a4ddfe17-0745-41d4-e104-25d1bd6d8da4"
      },
      "source": [
        "!git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "nothing to commit, working tree clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quQXD-E4vWVu"
      },
      "source": [
        "!git add ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "higMWzpfvgkM"
      },
      "source": [
        "!git config --global user.email \"mnmnalmagly@gmail.com\"\n",
        "!git config --global user.name \"AMNAALMGLY\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtXm7LzPvlYp",
        "outputId": "05e9003c-009e-47f3-edb3-1391e72ddd3f"
      },
      "source": [
        "!git commit -m 'first commit'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "nothing to commit, working tree clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2MzsB8_v0Q4"
      },
      "source": [
        "!git remote remove origin "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmHk-qzQzFyA"
      },
      "source": [
        "!git remote add origin https://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@github.com/AMNAALMGLY/AxiomaticDeepNets.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzSfzT5av5Q6",
        "outputId": "4a45ea49-8b1b-4a6e-9ae8-72912a133d32"
      },
      "source": [
        "!git branch -M main\n",
        "!git push -u origin main --force "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting objects: 41, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (40/40), done.\n",
            "Writing objects: 100% (41/41), 33.24 MiB | 11.43 MiB/s, done.\n",
            "Total 41 (delta 2), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (2/2), done.\u001b[K\n",
            "To https://github.com/AMNAALMGLY/AxiomaticDeepNets.git\n",
            " + 1003ecd...ea3d441 main -> main (forced update)\n",
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}